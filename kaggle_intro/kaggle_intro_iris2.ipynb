{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family']='SimHei' #顯示中文\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('input/train.csv', encoding = \"utf-8\", dtype = {'type': np.int32})\n",
    "test = pd.read_csv('input/test.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#把示範用的 type 4, 資料去除, 以免干擾建模\n",
    "train = train[train['type']!=4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train[['花瓣寬度','花瓣長度','花萼寬度','花萼長度']]\n",
    "y = train['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[[14  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "svc = SVC(C=1.0, kernel=\"rbf\", probability=True)\n",
    "svc.fit(X_train_std, y_train)\n",
    "\n",
    "print(metrics.classification_report(y_test, svc.predict(X_test_std)))\n",
    "print(metrics.confusion_matrix(y_test, svc.predict(X_test_std)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "<p>scikit-learn example: http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html</p>\n",
    "<p>scikit-learn    官網: http://scikit-learn.org/stable/</p>\n",
    "<p>         中文教學說明: https://www.gitbook.com/book/htygithub/machine-learning-python/details </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for < precision > -------------------------\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "mean_test_score (+/- std_test_score) for { paramaters..}\n",
      "\n",
      "0.119 (+/-0.011) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.119 (+/-0.011) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.940 (+/-0.105) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.119 (+/-0.011) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.947 (+/-0.119) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.940 (+/-0.105) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.927 (+/-0.129) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.947 (+/-0.119) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.934 (+/-0.127) for {'C': 1, 'kernel': 'linear'}\n",
      "0.937 (+/-0.105) for {'C': 10, 'kernel': 'linear'}\n",
      "0.921 (+/-0.134) for {'C': 100, 'kernel': 'linear'}\n",
      "0.921 (+/-0.134) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for < recall > -------------------------\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "mean_test_score (+/- std_test_score) for { paramaters..}\n",
      "\n",
      "0.333 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.907 (+/-0.170) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.333 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.944 (+/-0.122) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.907 (+/-0.170) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.923 (+/-0.134) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.944 (+/-0.122) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.933 (+/-0.130) for {'C': 1, 'kernel': 'linear'}\n",
      "0.934 (+/-0.109) for {'C': 10, 'kernel': 'linear'}\n",
      "0.921 (+/-0.136) for {'C': 100, 'kernel': 'linear'}\n",
      "0.921 (+/-0.136) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for < %s > -------------------------\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print(\"mean_test_score (+/- std_test_score) for { paramaters..}\")\n",
    "    print(\"\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier comparison\n",
    "<p>ref. https://machine-learning-python.kspax.io/Classification/ex4_Classifier_comparison.html</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\",\n",
    "         \"Random Forest\", \"AdaBoost\", \"Naive Bayes\", \"Linear Discriminant Ana.\",\n",
    "         \"Quadratic Discriminant Ana.\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "    SVC(kernel='rbf',C=100, gamma=0.001, probability=True),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Nearest Neighbors' score: 0.9722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'Linear SVM' score: 0.8889\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       0.80      0.80      0.80        10\n",
      "          3       0.83      0.83      0.83        12\n",
      "\n",
      "avg / total       0.89      0.89      0.89        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 3 2 3 2 2 2 3 2 2 2 3 2 3 3 3 3 2 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'RBF SVM' score: 0.9722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'Decision Tree' score: 0.9722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       0.91      1.00      0.95        10\n",
      "          3       1.00      0.92      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'Random Forest' score: 0.9722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'AdaBoost' score: 1.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      1.00      1.00        10\n",
      "          3       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'Naive Bayes' score: 0.9722\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 3 2 3 2 2 2 3 2 2 2 3 3 3 3 3 3 2 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'Linear Discriminant Ana.' score: 1.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      1.00      1.00        10\n",
      "          3       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n",
      "--------------------------------------------------------------\n",
      "'Quadratic Discriminant Ana.' score: 1.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      1.00      1.00        10\n",
      "          3       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        36\n",
      "\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X = train[['花瓣寬度','花瓣長度','花萼寬度','花萼長度']]\n",
    "y = train['type']\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=100)\n",
    "\n",
    "test_std = sc.transform(test[['花瓣寬度','花瓣長度','花萼寬度','花萼長度']])\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(\"%r score: %0.04f\" % (name, score))\n",
    "    \n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(clf.predict(test_std))\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score: 0.944444444444\n",
      "RF Score: 0.972222222222\n",
      "GNB Score: 0.972222222222\n",
      "SVC Score: 0.972222222222\n",
      "AdaBoost Score: 1.0\n",
      "L-Discr. Score: 1.0\n",
      "Q-Discr. Score: 1.0\n",
      "XGBoost Score: 0.972222222222\n",
      "Stacking Score: 0.972222222222\n",
      "--------------------------------------------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 3 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
    "clf2 = RandomForestClassifier(n_estimators=500, criterion='gini', max_features='auto', oob_score=True)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = SVC(kernel='rbf',C=100, gamma=0.001, probability=True)\n",
    "clf5 = AdaBoostClassifier()\n",
    "clf6 = LinearDiscriminantAnalysis()\n",
    "clf7 = QuadraticDiscriminantAnalysis()\n",
    "clf8 = xgb.XGBClassifier(n_estimators= 2000, max_depth= 4)\n",
    "meta_clf = LogisticRegression()\n",
    "stacking_clf = StackingClassifier(classifiers=[clf1, clf2, clf3, clf4, clf5, clf6, clf7, clf8], meta_classifier=meta_clf)\n",
    "\n",
    "clf1.fit(X_train_std, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "clf3.fit(X_train_std, y_train)\n",
    "clf4.fit(X_train_std, y_train)\n",
    "clf5.fit(X_train_std, y_train)\n",
    "clf6.fit(X_train_std, y_train)\n",
    "clf7.fit(X_train_std, y_train)\n",
    "clf8.fit(X_train_std, y_train)\n",
    "stacking_clf.fit(X_train_std, y_train)\n",
    "\n",
    "print('KNN Score:',clf1.score(X_test_std, y_test))\n",
    "print('RF Score:',clf2.score(X_test, y_test))\n",
    "print('GNB Score:',clf3.score(X_test_std, y_test))\n",
    "print('SVC Score:',clf4.score(X_test_std, y_test))\n",
    "print('AdaBoost Score:',clf5.score(X_test_std, y_test))\n",
    "print('L-Discr. Score:',clf6.score(X_test_std, y_test))\n",
    "print('Q-Discr. Score:',clf7.score(X_test_std, y_test))\n",
    "print('XGBoost Score:',clf8.score(X_test_std, y_test))\n",
    "print('Stacking Score:',stacking_clf.score(X_test_std, y_test))\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(stacking_clf.predict(test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L-Discr. Score: 1.0\n",
      "Q-Discr. Score: 1.0\n",
      "XGBoost Score: 0.972222222222\n",
      "Stacking Score: 1.0\n",
      "--------------------------------------------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = QuadraticDiscriminantAnalysis()\n",
    "clf3 = xgb.XGBClassifier(n_estimators= 2000, max_depth= 4)\n",
    "meta_clf = LinearDiscriminantAnalysis()\n",
    "stacking_clf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=meta_clf)\n",
    "\n",
    "clf1.fit(X_train_std, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "clf3.fit(X_train_std, y_train)\n",
    "stacking_clf.fit(X_train_std, y_train)\n",
    "\n",
    "print('L-Discr. Score:',clf1.score(X_test_std, y_test))\n",
    "print('Q-Discr. Score:',clf2.score(X_test_std, y_test))\n",
    "print('XGBoost Score:',clf3.score(X_test_std, y_test))\n",
    "print('Stacking Score:',stacking_clf.score(X_test_std, y_test))\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(stacking_clf.predict(test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      0.90      0.95        10\n",
      "          3       0.92      1.00      0.96        12\n",
      "\n",
      "avg / total       0.97      0.97      0.97        36\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
    "clf2 = RandomForestClassifier(n_estimators=500, criterion='gini', max_features='auto', oob_score=True)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = SVC(kernel='rbf',C=100, gamma=0.001, probability=True)\n",
    "clf5 = AdaBoostClassifier()\n",
    "clf6 = LinearDiscriminantAnalysis()\n",
    "clf7 = QuadraticDiscriminantAnalysis()\n",
    "clf8 = xgb.XGBClassifier(n_estimators= 2000, max_depth= 4)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('knn', clf1), ('rfc', clf2), ('gnb', clf3), ('svc', clf4),\n",
    "                                    ('Ada', clf5), ('Lda', clf6), ('Qda', clf7), ('XGB', clf8)], \n",
    "                        voting='hard', weights=[1, 1, 1, 1, 1, 1, 1, 1])\n",
    "eclf.fit(X_train_std, y_train)\n",
    "print(metrics.classification_report(y_test, eclf.predict(X_test_std)))\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(eclf.predict(test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        14\n",
      "          2       1.00      1.00      1.00        10\n",
      "          3       1.00      1.00      1.00        12\n",
      "\n",
      "avg / total       1.00      1.00      1.00        36\n",
      "\n",
      "--------------------------------------------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import xgboost as xgb\n",
    "\n",
    "clf1 = LinearDiscriminantAnalysis()\n",
    "clf2 = QuadraticDiscriminantAnalysis()\n",
    "clf3 = xgb.XGBClassifier(n_estimators= 2000, max_depth= 4)\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('Lda', clf1), ('Qda', clf2), ('XGB', clf3)], \n",
    "                        voting='hard', weights=[1, 1, 1])\n",
    "eclf.fit(X_train_std, y_train)\n",
    "print(metrics.classification_report(y_test, eclf.predict(X_test_std)))\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(eclf.predict(test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
